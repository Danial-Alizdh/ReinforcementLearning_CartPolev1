{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gym"
      ],
      "metadata": {
        "id": "V8-T_hZL7jaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a54502a-0f4e-4773-f169-359cec086a67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Collecting gym\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827616 sha256=e3a5127406aa49d9edf17e1b2767520389ed581351de608fd911fb217287aa8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.version import VERSION\n",
        "print(VERSION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dxspykb7RIY",
        "outputId": "06d0dc0f-2e90-491e-8123-b8b93a5be16c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.26.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HXgevJWX6noU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.optim import Adam, Optimizer\n",
        "import numpy as np\n",
        "import gym\n",
        "from matplotlib import animation\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ePZt4vMt6noV"
      },
      "outputs": [],
      "source": [
        "def create_model(number_observation_features: int, number_actions: int) -> nn.Module:\n",
        "    \"\"\"Create the MLP model\n",
        "\n",
        "    Args:\n",
        "        number_observation_features (int): Number of features in the (flat)\n",
        "        observation tensor\n",
        "        number_actions (int): Number of actions\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: Simple MLP model\n",
        "    \"\"\"\n",
        "    hidden_layer_features = 32\n",
        "\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(in_features=number_observation_features,\n",
        "                  out_features=hidden_layer_features),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_layer_features,\n",
        "                  out_features=number_actions),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x2AM1u8q6noV"
      },
      "outputs": [],
      "source": [
        "def get_policy(model: nn.Module, observation: np.ndarray) -> Categorical:\n",
        "    \"\"\"Get the policy from the model, for a specific observation\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): MLP model\n",
        "        observation (np.ndarray): Environment observation\n",
        "\n",
        "    Returns:\n",
        "        Categorical: Multinomial distribution parameterized by model logits\n",
        "    \"\"\"\n",
        "    observation_tensor = torch.as_tensor(observation, dtype=torch.float32)\n",
        "    logits = model(observation_tensor)\n",
        "\n",
        "    # Categorical will also normalize the logits for us\n",
        "    return Categorical(logits=logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wFRRGQQW6noW"
      },
      "outputs": [],
      "source": [
        "def get_action(policy: Categorical) -> tuple[int, float]:\n",
        "    \"\"\"Sample an action from the policy\n",
        "\n",
        "    Args:\n",
        "        policy (Categorical): Policy\n",
        "\n",
        "    Returns:\n",
        "        tuple[int, float]: Tuple of the action and it's log probability\n",
        "    \"\"\"\n",
        "    action = policy.sample()  # Unit tensor\n",
        "\n",
        "    # Converts to an int, as this is what Gym environments require\n",
        "    action_int = action.item()\n",
        "\n",
        "    # Calculate the log probability of the action, which is required for\n",
        "    # calculating the loss later\n",
        "    log_probability_action = policy.log_prob(action)\n",
        "\n",
        "    return action_int, log_probability_action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_UhBeSWR6noW"
      },
      "outputs": [],
      "source": [
        "def calculate_loss(epoch_log_probability_actions: torch.Tensor, epoch_action_rewards: torch.Tensor) -> float:\n",
        "    \"\"\"Calculate the 'loss' required to get the policy gradient\n",
        "\n",
        "    Formula for gradient at\n",
        "    https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient\n",
        "\n",
        "    Note that this isn't really loss - it's just the sum of the log probability\n",
        "    of each action times the episode return. We calculate this so we can\n",
        "    back-propagate to get the policy gradient.\n",
        "\n",
        "    Args:\n",
        "        epoch_log_probability_actions (torch.Tensor): Log probabilities of the\n",
        "            actions taken\n",
        "        epoch_action_rewards (torch.Tensor): Rewards for each of these actions\n",
        "\n",
        "    Returns:\n",
        "        float: Pseudo-loss\n",
        "    \"\"\"\n",
        "    # return -(epoch_log_probability_actions * epoch_action_rewards).mean()\n",
        "\n",
        "    if epoch_log_probability_actions.size(0) != epoch_action_rewards.size(0):\n",
        "        # Pad or truncate to make the sizes along dimension 0 equal\n",
        "        min_size = min(epoch_log_probability_actions.size(0), epoch_action_rewards.size(0))\n",
        "        epoch_log_probability_actions = epoch_log_probability_actions[:min_size]\n",
        "        epoch_action_rewards = epoch_action_rewards[:min_size]\n",
        "\n",
        "    return -(epoch_log_probability_actions * epoch_action_rewards).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BclDRzso6noW"
      },
      "outputs": [],
      "source": [
        "frames = []\n",
        "\n",
        "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
        "\n",
        "    #Mess with this to change frame size\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
        "\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    anim.save(path + filename, writer='imagemagick', fps=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "R5M0Jw7R6noW"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(env: gym.Env, model: nn.Module, optimizer: Optimizer, max_timesteps=100, episode_timesteps=200) -> float:\n",
        "    \"\"\"Train the model for one epoch\n",
        "\n",
        "    Args:\n",
        "        env (gym.Env): Gym environment\n",
        "        model (nn.Module): Model\n",
        "        optimizer (Optimizer): Optimizer\n",
        "        max_timesteps (int, optional): Max timesteps per epoch. Note if an\n",
        "            episode is part-way through, it will still complete before finishing\n",
        "            the epoch. Defaults to 5000.\n",
        "        episode_timesteps (int, optional): Timesteps per episode. Defaults to 200.\n",
        "\n",
        "    Returns:\n",
        "        float: Average return from the epoch\n",
        "    \"\"\"\n",
        "    epoch_total_timesteps = 0\n",
        "\n",
        "    # Returns from each episode (to keep track of progress)\n",
        "    epoch_returns: list[int] = []\n",
        "\n",
        "    # Action log probabilities and rewards per step (for calculating loss)\n",
        "    epoch_log_probability_actions = []\n",
        "    epoch_action_rewards = []\n",
        "\n",
        "    # Loop through episodes\n",
        "    while True:\n",
        "\n",
        "        # Stop if we've done over the total number of timesteps\n",
        "        if epoch_total_timesteps > max_timesteps:\n",
        "            break\n",
        "\n",
        "        # Running total of this episode's rewards\n",
        "        episode_reward: int = 0\n",
        "\n",
        "        # Reset the environment and get a fresh observation\n",
        "        observation, _ = env.reset()\n",
        "\n",
        "        # Loop through timesteps until the episode is done (or the max is hit)\n",
        "        for timestep in range(episode_timesteps):\n",
        "            # env.render()\n",
        "            epoch_total_timesteps += 1\n",
        "\n",
        "            # Get the policy and act\n",
        "            policy = get_policy(model, observation)\n",
        "            action, log_probability_action = get_action(policy)\n",
        "\n",
        "            # env.render()\n",
        "            frames.append(env.render())\n",
        "\n",
        "            observation, reward, done, _, _= env.step(action)\n",
        "\n",
        "            # Increment the episode rewards\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Add epoch action log probabilities\n",
        "            epoch_log_probability_actions.append(log_probability_action)\n",
        "\n",
        "            # Finish the action loop if this episode is done\n",
        "            if done == True:\n",
        "                # Add one reward per timestep\n",
        "                for _ in range(timestep + 1):\n",
        "                    epoch_action_rewards.append(episode_reward)\n",
        "\n",
        "                break\n",
        "\n",
        "        # Increment the epoch returns\n",
        "        epoch_returns.append(episode_reward)\n",
        "\n",
        "    # Calculate the policy gradient, and use it to step the weights & biases\n",
        "    epoch_loss = calculate_loss(torch.stack(\n",
        "        epoch_log_probability_actions),\n",
        "        torch.as_tensor(\n",
        "        epoch_action_rewards, dtype=torch.float32)\n",
        "    )\n",
        "\n",
        "    epoch_loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    return np.mean(epoch_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8AXdBWXl6noX",
        "outputId": "2fce4e6e-b80d-43a3-e03c-f50970567f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:   0 \t return: 27.500\n",
            "epoch:   1 \t return: 18.167\n",
            "epoch:   2 \t return: 22.200\n",
            "epoch:   3 \t return: 20.800\n",
            "epoch:   4 \t return: 16.286\n",
            "epoch:   5 \t return: 22.400\n",
            "epoch:   6 \t return: 26.250\n",
            "epoch:   7 \t return: 18.833\n",
            "epoch:   8 \t return: 26.800\n",
            "epoch:   9 \t return: 16.833\n",
            "epoch:  10 \t return: 27.000\n",
            "epoch:  11 \t return: 23.200\n",
            "epoch:  12 \t return: 21.000\n",
            "epoch:  13 \t return: 27.500\n",
            "epoch:  14 \t return: 41.000\n",
            "epoch:  15 \t return: 25.250\n",
            "epoch:  16 \t return: 22.200\n",
            "epoch:  17 \t return: 22.200\n",
            "epoch:  18 \t return: 35.750\n",
            "epoch:  19 \t return: 29.500\n",
            "epoch:  20 \t return: 32.250\n",
            "epoch:  21 \t return: 17.000\n",
            "epoch:  22 \t return: 17.333\n",
            "epoch:  23 \t return: 49.333\n",
            "epoch:  24 \t return: 22.000\n",
            "epoch:  25 \t return: 42.000\n",
            "epoch:  26 \t return: 31.000\n",
            "epoch:  27 \t return: 21.800\n",
            "epoch:  28 \t return: 25.250\n",
            "epoch:  29 \t return: 27.500\n",
            "epoch:  30 \t return: 27.200\n",
            "epoch:  31 \t return: 37.000\n",
            "epoch:  32 \t return: 45.750\n",
            "epoch:  33 \t return: 40.000\n",
            "epoch:  34 \t return: 24.333\n",
            "epoch:  35 \t return: 37.750\n",
            "epoch:  36 \t return: 29.000\n",
            "epoch:  37 \t return: 37.000\n",
            "epoch:  38 \t return: 23.400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.animation:MovieWriter imagemagick unavailable; using Pillow instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  39 \t return: 23.400\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def train(epochs=40) -> None:\n",
        "    \"\"\"Train a Vanilla Policy Gradient model on CartPole\n",
        "\n",
        "    Args:\n",
        "        epochs (int, optional): The number of epochs to run for. Defaults to 50.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the Gym Environment\n",
        "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    env.action_space.seed(0)\n",
        "\n",
        "    # Create the MLP model\n",
        "    number_observation_features = env.observation_space.shape[0]\n",
        "    number_actions = env.action_space.n\n",
        "    model = create_model(number_observation_features, number_actions)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = Adam(model.parameters(), 1e-2)\n",
        "\n",
        "    # Loop for each epoch\n",
        "    for epoch in range(epochs):\n",
        "        average_return = train_one_epoch(env, model, optimizer)\n",
        "        print('epoch: %3d \\t return: %.3f' % (epoch, average_return))\n",
        "\n",
        "    env.close()\n",
        "    save_frames_as_gif(frames)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}